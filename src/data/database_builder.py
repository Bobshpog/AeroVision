import osimport sysfrom dataclasses import dataclass, fieldfrom datetime import datetimefrom pathlib import Pathfrom typing import Unionimport h5pyimport numpy as npfrom tqdm import tqdmfrom src.data.data_generators.data_gen import DataGenerator@dataclassclass DatabaseBuilder:    data_generator: DataGenerator    db_folder: Union[str, Path] = field(repr=False)    batch_size: int=field(repr=False,default=200)    compression: str = 'gzip'    def __post_init__(self):        if isinstance(self.db_folder, str):            self.db_folder = Path(self.db_folder)        if not self.db_folder.exists():            os.mkdir(self.db_folder)    def __call__(self, comment=None, dtype=np.float32):        """       Creates a database from data in self.raw_data_folder        Args:            comment: additional comment for name            dtype: data type of numbers in dataset        Returns:            path to the newly created database        """        if not comment:            comment= ''        comment = self.db_folder / f"{datetime.now().strftime('%Y%m%d-%H%M%S')}__{self.data_generator}__{comment}.hdf5"        num_datapoints = len(self.data_generator)        num_scales, image_shape, num_ir = self.data_generator.get_data_sizes()        with h5py.File(str(comment.absolute()), 'w') as hf:            self.data_generator.save_metadata(hf, 'generator metadata')            data_grp = hf.create_group('data')            dset_images = data_grp.create_dataset('images',                                                  shape=(num_datapoints, *image_shape),                                                  maxshape=(num_datapoints, *image_shape),                                                  chunks=(1, 1, *(image_shape[1:])),                                                  compression=self.compression,                                                  dtype=dtype)            dset_scales = data_grp.create_dataset('scales',                                                  shape=(num_datapoints, num_scales),                                                  maxshape=(num_datapoints, num_scales),                                                  chunks=(1, num_scales),                                                  compression=self.compression,                                                  dtype=np.float64)            dset_video_names = data_grp.create_dataset('video_names',                                                       shape=(num_datapoints,),                                                       maxshape=(num_datapoints,),                                                       compression=self.compression,                                                       dtype=h5py.string_dtype(encoding='ascii'))            progress_bar = tqdm(enumerate(self.data_generator), desc='Building Database', total=num_datapoints,                                file=sys.stdout)            names, images, ir, scales = [], [], [], []            cache_idx = 0            for idx, datapoint in progress_bar:                # datapoint =(video_name, image, ir, scales)                # print(idx, points_path, scales_path)                names.append(datapoint[0]), images.append(datapoint[1]), ir.append(datapoint[2]), scales.append(                    datapoint[3])                if idx % self.batch_size== 0:                    dset_video_names[cache_idx:idx + 1] = names                    dset_images[cache_idx:idx + 1] = images                    dset_scales[cache_idx:idx + 1] = scales                    cache_idx = idx + 1                    names, images, ir, scales = [], [], [], []                    pass            dset_video_names[cache_idx:] = names            dset_images[cache_idx:] = images            dset_scales[cache_idx:] = scales            return comment